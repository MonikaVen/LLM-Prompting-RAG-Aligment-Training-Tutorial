{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonikaVen/LLM-Prompting-RAG-Aligment-Training-Tutorial/blob/main/2-Mistral-7B-RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
      },
      "source": [
        "### Necessary imports"
      ],
      "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
      ],
      "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
      },
      "source": [
        "### Dependencies"
      ],
      "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain"
      ],
      "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
      },
      "source": [
        "### Load quantized Mistal 7B"
      ],
      "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80f94e97-7e58-4253-9376-73af6f36e139"
      },
      "outputs": [],
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "id": "80f94e97-7e58-4253-9376-73af6f36e139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
      },
      "source": [
        "### Count number of trainable parameters"
      ],
      "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91d2a86e-69e8-496f-b388-853168537c20"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ],
      "id": "91d2a86e-69e8-496f-b388-853168537c20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
      },
      "source": [
        "### Build Mistral text generation pipeline"
      ],
      "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        ")"
      ],
      "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
      },
      "outputs": [],
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
      },
      "source": [
        "### Load and chunk documents. Load chunked documents into FAISS index"
      ],
      "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
      },
      "outputs": [],
      "source": [
        "!playwright install\n",
        "!playwright install-deps"
      ],
      "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles to index\n",
        "articles = [\"https://pycon.lt/2024/tickets/\",\n",
        "            \"https://pycon.lt/2024/talks/\",\n",
        "            \"https://pycon.lt/2024/schedule/\",\n",
        "            \"https://pycon.lt/2024/social-events/\",\n",
        "            \"https://pycon.lt/2024/jobs/\"]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()"
      ],
      "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
      },
      "outputs": [],
      "source": [
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100,\n",
        "                                      chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6"
      },
      "source": [
        "### Create PromptTemplate and LLMChain"
      ],
      "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your  PyCon LT knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ],
      "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "727e50d1-5739-4a65-8745-aaa4c9c47189"
      },
      "outputs": [],
      "source": [
        "llm_chain.invoke({\"context\": \"\", \"question\": \"What is an evening event at PyCon LT?\"})"
      ],
      "id": "727e50d1-5739-4a65-8745-aaa4c9c47189"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1e75d34-cf63-49a8-a671-88ccb5444367"
      },
      "source": [
        "### Build RAG Chain"
      ],
      "id": "c1e75d34-cf63-49a8-a671-88ccb5444367"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1e18178-46f2-4b87-86c4-d13ff5219968"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What is an evening event at PyCon LT?\")"
      ],
      "id": "a1e18178-46f2-4b87-86c4-d13ff5219968"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef54e9e3-4fa5-4676-baaf-8b11e3f09dc0"
      },
      "outputs": [],
      "source": [
        "result['context']"
      ],
      "id": "ef54e9e3-4fa5-4676-baaf-8b11e3f09dc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62895f39-9dfb-4f58-8312-72580ca03a20"
      },
      "outputs": [],
      "source": [
        "print(result['text'])"
      ],
      "id": "62895f39-9dfb-4f58-8312-72580ca03a20"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "common-gpu.m114",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}