{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonikaVen/LLM-Prompting-RAG-Aligment-Training-Tutorial/blob/main/2-Mistral-7B-RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
      },
      "source": [
        "### Necessary imports"
      ],
      "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
      ],
      "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
      },
      "source": [
        "### Dependencies"
      ],
      "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain"
      ],
      "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
      },
      "source": [
        "### Load quantized Mistal 7B"
      ],
      "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "80f94e97-7e58-4253-9376-73af6f36e139",
        "outputId": "6614db66-d3ae-4373-c038-fa6793751c28"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-55652fc43722>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Check GPU compatibility with bfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompute_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "id": "80f94e97-7e58-4253-9376-73af6f36e139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
      },
      "source": [
        "### Count number of trainable parameters"
      ],
      "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91d2a86e-69e8-496f-b388-853168537c20"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ],
      "id": "91d2a86e-69e8-496f-b388-853168537c20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
      },
      "source": [
        "### Build Mistral text generation pipeline"
      ],
      "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        ")"
      ],
      "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
      },
      "outputs": [],
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
      },
      "source": [
        "### Load and chunk documents. Load chunked documents into FAISS index"
      ],
      "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
      },
      "outputs": [],
      "source": [
        "!playwright install\n",
        "!playwright install-deps"
      ],
      "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles to index\n",
        "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
        "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
        "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
        "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
        "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()"
      ],
      "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
      },
      "outputs": [],
      "source": [
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100,\n",
        "                                      chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6"
      },
      "source": [
        "### Create PromptTemplate and LLMChain"
      ],
      "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your fantasy football knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ],
      "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "727e50d1-5739-4a65-8745-aaa4c9c47189"
      },
      "outputs": [],
      "source": [
        "llm_chain.invoke({\"context\": \"\", \"question\": \"Should I start Gibbs in week 16 for fantasy?\"})"
      ],
      "id": "727e50d1-5739-4a65-8745-aaa4c9c47189"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1e75d34-cf63-49a8-a671-88ccb5444367"
      },
      "source": [
        "### Build RAG Chain"
      ],
      "id": "c1e75d34-cf63-49a8-a671-88ccb5444367"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1e18178-46f2-4b87-86c4-d13ff5219968"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"Should I start Gibbs next week for fantasy?\")"
      ],
      "id": "a1e18178-46f2-4b87-86c4-d13ff5219968"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef54e9e3-4fa5-4676-baaf-8b11e3f09dc0"
      },
      "outputs": [],
      "source": [
        "result['context']"
      ],
      "id": "ef54e9e3-4fa5-4676-baaf-8b11e3f09dc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62895f39-9dfb-4f58-8312-72580ca03a20"
      },
      "outputs": [],
      "source": [
        "print(result['text'])"
      ],
      "id": "62895f39-9dfb-4f58-8312-72580ca03a20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63253cbc-9de3-41b9-a5c8-b672333f479c"
      },
      "outputs": [],
      "source": [],
      "id": "63253cbc-9de3-41b9-a5c8-b672333f479c"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "common-gpu.m114",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}