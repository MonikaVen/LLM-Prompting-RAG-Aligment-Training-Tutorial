{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rrj1MCpaYhRZ",
        "outputId": "8ee9ef2a-af9f-447c-a82e-08c642e4cd45"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv==1.0.1 nest-asyncio==1.6.0 llama-index==0.10.45 requests==2.32.3 googletrans==2.2.0 py-trans==0.6.1 aiofiles==23.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xii1igyjZJ2v"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "def get_router_query_engine(file_path: str, llm = None, embed_model = None):\n",
        "    \"\"\"Get router query engine.\"\"\"\n",
        "    llm = llm or OpenAI(model=\"gpt-3.5-turbo\")\n",
        "    embed_model = embed_model or OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "    # load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
        "\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "        llm=llm\n",
        "    )\n",
        "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            \"Useful for summarization questions related to doc\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=(\n",
        "            \"Useful for retrieving specific context from the doc\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    query_engine = RouterQueryEngine(\n",
        "        selector=LLMSingleSelector.from_defaults(),\n",
        "        query_engine_tools=[\n",
        "            summary_tool,\n",
        "            vector_tool,\n",
        "        ],\n",
        "        verbose=True\n",
        "    )\n",
        "    return query_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzmCjR3nZUaC",
        "outputId": "b51824a3-4d41-4849-97bc-dfe88fe5cf70"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "OPENAI_API_KEY = \"\"\n",
        "\n",
        "print(\"Read the OPENAI_API_KEY\")\n",
        "#The openai library will automatically use the key if it is set in the OPENAI_API_KEY environment variable\n",
        "#so we set it here\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"xLSTM.pdf\"]).load_data()\n",
        "\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyjQHM5_c13O"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to doc\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the doc.\"\n",
        "    ),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjbPqFktc66D",
        "outputId": "558b7c96-8e2e-42ee-8a24-bcbd1ca4c1d6"
      },
      "outputs": [],
      "source": [
        "#Define router query engine\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"How xLSTM is better than LSTM for time series data?\"\n",
        ")\n",
        "print(str(response))\n",
        "response = query_engine.query(\n",
        "    \"Is their memory configurable? How gating does work?\"\n",
        ")\n",
        "print(str(response))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMEXt1CPdBub",
        "outputId": "1e9bd04f-729c-43df-a65b-b17e3e4b8068"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Parašyk dokumento santrauką.\")\n",
        "print(str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXGwtOGPdDbO",
        "outputId": "6f757671-da95-4e89-c68d-90bbeaaa2674"
      },
      "outputs": [],
      "source": [
        "from py_trans import PyTranslator\n",
        "\n",
        "tr = PyTranslator()\n",
        "\n",
        "resp_str = str(response)\n",
        "print(tr.google(resp_str, \"lt\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
