{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOmTPNhqztO4AzzxRq2Nd0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dca3825a86524de09fcddbfb18407647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2290d88b0f3b4c50bbf039a3c7d3e0cd",
              "IPY_MODEL_99f35d0fcc6140d6b11f2c9042607362",
              "IPY_MODEL_e35ce823d2404c278be87214563fb942"
            ],
            "layout": "IPY_MODEL_1e0e874e79a7418b9ff0a9a826d5d5e5"
          }
        },
        "2290d88b0f3b4c50bbf039a3c7d3e0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce61877880e24797a042fdca2735566f",
            "placeholder": "​",
            "style": "IPY_MODEL_9c0b47eaecff49aaa214d924c41d89ef",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "99f35d0fcc6140d6b11f2c9042607362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2d395e5b0b646e99a04c435b16856c9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8fee0f548c14396a35e0a582b848bad",
            "value": 2
          }
        },
        "e35ce823d2404c278be87214563fb942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afe3c794247648a6a81e4a6d6eafe816",
            "placeholder": "​",
            "style": "IPY_MODEL_0a9cc0f034f44af8a3f82c34a6c7b70d",
            "value": " 2/2 [01:01&lt;00:00, 28.55s/it]"
          }
        },
        "1e0e874e79a7418b9ff0a9a826d5d5e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce61877880e24797a042fdca2735566f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c0b47eaecff49aaa214d924c41d89ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2d395e5b0b646e99a04c435b16856c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fee0f548c14396a35e0a582b848bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afe3c794247648a6a81e4a6d6eafe816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a9cc0f034f44af8a3f82c34a6c7b70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonikaVen/LLM-Prompting-RAG-Aligment-Training-Tutorial/blob/main/1-Mistral-7B-Effective-Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Effective Prompting With Mistral 7B\n",
        "Try different metatags.\n",
        "Try Chain-of-Thought prompting\n",
        "\n"
      ],
      "metadata": {
        "id": "JLCyw1qoYmn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
        "\n",
        "https://huggingface.co/mistralai/Mistral-7B-v0.1/tree/main\n",
        "\n",
        "https://www.promptingguide.ai/models/mistral-7b\n",
        "\n",
        "https://arxiv.org/pdf/2310.06825.pdf\n",
        "\n",
        "https://docs.mistral.ai/platform/guardrailing/"
      ],
      "metadata": {
        "id": "FhZwnARm-oQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You only need to run this once per machine\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAaWIKNw1bRS",
        "outputId": "8a4a1f53-35fc-4aca-96c9-bbf71b285cb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "TD5iN1G6bFtz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load and Test Mistral 7B"
      ],
      "metadata": {
        "id": "6o733tNeZKYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251,
          "referenced_widgets": [
            "dca3825a86524de09fcddbfb18407647",
            "2290d88b0f3b4c50bbf039a3c7d3e0cd",
            "99f35d0fcc6140d6b11f2c9042607362",
            "e35ce823d2404c278be87214563fb942",
            "1e0e874e79a7418b9ff0a9a826d5d5e5",
            "ce61877880e24797a042fdca2735566f",
            "9c0b47eaecff49aaa214d924c41d89ef",
            "a2d395e5b0b646e99a04c435b16856c9",
            "d8fee0f548c14396a35e0a582b848bad",
            "afe3c794247648a6a81e4a6d6eafe816",
            "0a9cc0f034f44af8a3f82c34a6c7b70d"
          ]
        },
        "id": "WQ0Km1nR02X0",
        "outputId": "f6ce5193-94f1-469b-f647-10a1cf438972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dca3825a86524de09fcddbfb18407647"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> def print_prime(n):\\n   \"\"\"\\n   Print all primes between 1 and n\\n   \"\"\"\\n   for i in range(2, n):\\n      if is_prime(i):\\n         print(i)\\n\\ndef is_prime(n):\\n   \"\"\"\\n   Check if n is prime\\n   \"\"\"\\n   for i in range(2, n):\\n      if n % i == 0:\\n         return False\\n   return True\\n\\nprint_prime(100)</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "inputs = tokenizer('''def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "tokenizer.batch_decode(outputs)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try different metatags for Mistral 7B"
      ],
      "metadata": {
        "id": "pcrklX7jZjwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def evaluate_prompt(prompt, tokenizer):\n",
        "    with torch.no_grad():\n",
        "        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    generated_code = tokenizer.decode(model.generate(**model_inputs, max_new_tokens=256)[0], skip_special_tokens=True)\n",
        "    return generated_code\n",
        "\n",
        "print('-------------------------------------------------------------')\n",
        "eval_prompt = \"\"\" Print hello world in python c and c++ \"\"\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "result = evaluate_prompt(eval_prompt, tokenizer)"
      ],
      "metadata": {
        "id": "zfBmxGm1J0m5",
        "outputId": "e16f8401-4c65-4fce-cea7-624212952e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Using prompt: Print hello world in python c and c++ , prompt result: \", result )\n",
        "print('-------------------------------------------------------------')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "eval_prompt = \"\"\"[INST] Print hello world in python c and c++ [/INST]\"\"\"\n",
        "result = evaluate_prompt(eval_prompt, tokenizer)\n",
        "print(f\"Using prompt: [INST] Print hello world in python c and c++ [/INST], prompt result: \", result )"
      ],
      "metadata": {
        "id": "4tgbxqhHtNY4",
        "outputId": "4a524cc1-97e1-4991-b447-92d4255041eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using prompt: Print hello world in python c and c++ , prompt result:   Print hello world in python c and c++ 1.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------\n",
            "Using prompt: [INST] Print hello world in python c and c++ [/INST], prompt result:  [INST] Print hello world in python c and c++ [/INST]\n",
            "\n",
            "# [INST] Print hello world in python c and c++\n",
            "\n",
            "## [INST] Print hello world in python c and c++\n",
            "\n",
            "### [INST] Print hello world in python c and c++\n",
            "\n",
            "#### [INST] Print hello world in python c and c++\n",
            "\n",
            "##### [INST] Print hello world in python c and c++\n",
            "\n",
            "###### [INST] Print hello world in python c and c++\n",
            "\n",
            "```\n",
            "#include <stdio.h>\n",
            "\n",
            "int main() {\n",
            "    printf(\"Hello, World!\\n\");\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "```\n",
            "#include <iostream>\n",
            "\n",
            "int main() {\n",
            "    std::cout << \"Hello, World!\" << std::endl;\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "```\n",
            "#!/usr/bin/env python\n",
            "\n",
            "print(\"Hello, World!\")\n",
            "```\n",
            "\n",
            "```\n",
            "#!/usr/bin/env python\n",
            "\n",
            "print(\"Hello, World!\")\n",
            "```\n",
            "\n",
            "```\n",
            "#!/usr/bin/env python\n",
            "\n",
            "print(\"Hello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Mistral 7B Instruct and test Chain-of-Thought Prompting"
      ],
      "metadata": {
        "id": "DKhfuVQNZ4F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages_1 = [\n",
        "    {\"role\": \"user\", \"content\": \"[INST] The Cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they still have? [/INST]\"},\n",
        "]\n",
        "\n",
        "model_inputs = tokenizer.apply_chat_template(messages_1, return_tensors=\"pt\").to(device)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=400, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "id": "9WHJkraeXTZI",
        "outputId": "4e0528b0-3116-4c49-9715-b550008e6808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
            "\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [INST] [INST] The Cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they still have? [/INST] [/INST] [HIDE] [MULTIOPTIONAL]\\n\\n- 13\\n- 7\\n- 23\\n- 9\\n- 10 is a fraction\\n\\n[/MULTIOPTIONAL]</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages_2 = [\n",
        "    {\"role\": \"user\", \"content\": \"The Cafeteria had 23 apples.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Understood that the Cafeteria had 23 apples.\"},\n",
        "    {\"role\": \"user\", \"content\": \"They used 20 to make lunch.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Understood that they have 23-20=3 apples left.\"},\n",
        "    {\"role\": \"user\", \"content\":  \"Then they bought 6 more. How many apples do they still have? \"}\n",
        "]\n",
        "\n",
        "model_inputs = tokenizer.apply_chat_template(messages_2, return_tensors=\"pt\").to(device)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=400, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "id": "5tEANtGdcKKQ",
        "outputId": "5a2f94f6-1d52-43d6-f821-3cda805cdb10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [INST] The Cafeteria had 23 apples. [/INST] Understood that the Cafeteria had 23 apples. </s><s> [INST] They used 20 to make lunch. [/INST] Understood that they have 23-20=3 apples left. </s><s> [INST] Then they bought 6 more. How many apples do they still have? [/INST] Understood that they need to count the apples and add 3+6 6+3=9 6+9=15 industries. To count things, or count from one thing to another, you can use the words: [INST] 1. [/INST] Understood that the words count can be used to count. 2. [IST] count up [/IST] Understood that the words count up can be used to count. 3. [INST] count down[/INST] Understood that the words count down can be used to count. 4. count off [INST] [/INST] Understood that the word count off can be used to count. 5. count on [INST] [/INST] Understood that the word count on can be used to count. 6. [INST] count to [/INST] Understood that the words count to can be used to count. 7. [INST] count up to [/INST] [INST] Understood that the words count up to can be used to count. 8. [INST] count down to [/INST] [INST] Understood that the words count down to can be used to count. 9. [INST] count off to [/INST] Understood that the word count off to can be used to count. 10. count in to[INST] [/INST] Understood that the words count in to can be used to count. 11. [INST] count one by one [/INST] Understood that the words count one by one can be used to count. 12. count on two [INST] [/INST] Understood that the words count on two can be used to count. 13. count up until 6 [INST] [/INST] Understood that the words to count up until 6 can be used to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}